---
title: "animal_crossing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r}
library(tidyverse)
theme_set(theme_light())

critic <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/critic.tsv')
user_reviews <- readr::read_tsv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/user_reviews.tsv')
items <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/items.csv')
villagers <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-05/villagers.csv')
```


```{r}
villagers
items %>%   View()
```


```{r}
critic %>%  
  ggplot(aes(grade))+
  geom_histogram()

user_reviews %>%  
  ggplot(aes(grade))+
  geom_histogram()

```


```{r}
library(tidytext)
library(lubridate)

by_week<-user_reviews %>% 
  group_by(week=floor_date(date, "week")) %>% 
  summarise(nb_reviews=n(),
            average_grade=mean(grade), 
            pct_zero=mean(grade==0),
            pct_ten=mean(grade==10)
            ) 

user_reviews_words<-user_reviews %>% 
  unnest_tokens(word, text) %>% 
  anti_join(stop_words, by="word") %>% 
  count(user_name, date, grade, word)
```

```{r}
user_reviews_words %>% 
  group_by(word) %>% 
  summarise(average_grade=mean(grade), 
            nb_reviews=n()) %>% 
  arrange(desc(nb_reviews)) %>% 
  filter(nb_reviews>=50) %>% 
  arrange(desc(average_grade))

```
#have the grades been getting more polarised?

```{r}
user_reviews %>% 
  count(grade) %>%  
  ggplot(aes(grade,n))+ 
  geom_col()
```

#lets do cut offs. >7.5 happy ppl and less is not that happy ppl. Lets build a model and predict which reviews are going into which width bins. lets see the data first. Happy and unnhappy ppl
```{r}
user_reviews %>% 
filter (grade>8) %>% 
sample_n(5) %>% 
pull(text)

reviews_parsed<-user_reviews %>% 
mutate(text=str_remove(text,"Expand$"),
      rating=case_when(grade>6~"good", 
                        TRUE~"bad"))
```

```{r}
library(tidytext)
words_per_review<-reviews_parsed %>% 
  unnest_tokens(word, text) %>% 
  count(user_name, name="total_words")
words_per_review %>% 
  ggplot(aes(total_words))+
  geom_histogram()
```

#build a model 

```{r}
library(tidymodels)
set.seed(123)
#splititing the data evenly
review_split<-initial_split(reviews_parsed,stata=rating)
review_train<-training(review_split)
review_test<-testing(review_split)
#we are going to use the text data to predict the rating
```

```{r}
library(textrecipes)

review_rec<-recipe(rating ~text, data=review_train) %>% 
  step_tokenize(text) %>% 
  step_stopwords(text) %>% 
  step_tokenfilter(text, max_tokens = 500) %>% 
  step_tfidf(text) %>% 
  step_normalize(all_predictors())
  
review_prep<-prep(review_rec)
review_prep
juice(review_prep)
```

#declare our model specification. We are training a classification model. we are going to train a logistic reg model with lasso regulatiozation. Doing this by saying mixture=1 
```{r}
lasso_spec<-logistic_reg(penalty = tune(), mixture = 1 ) %>% 
  set_engine("glmnet")

lasso_wf<-workflow() %>% 
  add_recipe(review_rec) %>% 
  add_model(lasso_spec)
```
#this is the model that we are going to fit to pretict what animal crossing reviews are positive/negative


```{r}
#tune model parameters 
lambda_grid<-grid_regular(penalty(), levels=30)
set.seed(123)
review_folds<- bootstraps(review_train, strata=rating)
#stratify resampling to balance analysis set and assement sets are balance in the user reviews 
review_folds
```

```{r}
#tune a grid
doParallel::registerDoParallel()

set.seed(2020)
lasso_grid<-tune_grid(
          lasso_wf, 
          resamples=review_folds,
          grid=lambda_grid, 
          metrics=metric_set(roc_auc, ppv, npv))
```

```{r}
lasso_grid %>% 
  collect_metrics() %>% 
  ggplot(aes(penalty, mean, color=.metric)) +
           geom_line(size=1, show.legend=FALSE) +
           facet_wrap(~.metric)+
           scale_x_log10()
         
```

#choose the final model
```{r}
best_auc<-lasso_grid %>% 
  select_best("roc_auc")

final_lasso<-finalize_workflow(lasso_wf, best_auc)

```

```{r}
library(vip)
final_lasso %>% 
  fit(review_train) %>%
  pull_workflow_fit() %>% 
  vi(lambda=best_auc$penalty) %>% 
  group_by(Sign) %>% 
  top_n(20, wt=abs(Importance)) %>% 
  ungroup() %>% 
  mutate(Importance=abs(Importance),
         Variable=str_remove(Variable, "tfidf_text_"), 
         Variable=fct_reorder(Variable, Importance)) %>% 
  ggplot(aes(x=Importance, y=Variable, fill=Sign))+
  geom_col(show.legend =FALSE)+
  facet_wrap(~Sign,scales="free_y")
  
```

#final fit 
```{r}
review_final<-last_fit(final_lasso, review_split)
#it fits to the training data and evaluates on the testing data
review_final %>% collect_metrics()
review_final %>% collect_predictions() %>% 
  conf_mat(rating, .pred_class )
#its easier to detect bad comments than good ones. 
#accuracy is pretty good. to increase it, ammend recipe, by including bigrams, selecting stopwirds thath make more sense 
```






